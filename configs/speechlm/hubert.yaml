dataset:
  wav_dir_train: "data/librilight"
  ext_audio: ".flac"

  unicode_train: "data/speechlm/hubert/unicode/train"
  train_file: "data/speechlm/hubert/unit/train.txt"
  units_per_sample: 128

  swuggy_dev_file: "data/speechlm/hubert/unit/lexical/dev.json"
  sblimp_dev_file: "data/speechlm/hubert/unit/syntactic/dev.json"
  swuggy_test_file: "data/speechlm/hubert/unit/lexical/test.json"
  sblimp_test_file: "data/speechlm/hubert/unit/syntactic/test.json"

  swuggy_dir: "~/zr-data/datasets/sLM21-dataset/lexical"
  sblimp_dir: "~/zr-data/datasets/sLM21-dataset/syntactic"
  result_dir: "results/speechlm/hubert"

dataloader:
  batch_size_per_device: 96  # effective batch size (tokens) = dataset.units_per_sample * batch_size_per_device * #GPUs
  num_workers: 16

model:
  path: "models/speechlm/hubert"
  vocab_size: 16384  # BPE vocab size
  hidden_size: 768
  intermediate_size: 3072
  num_hidden_layers: 12
  num_attention_heads: 12
  pad_token_id: 0
  bos_token_id: null
  eos_token_id: 1  # for generation stopping criteria

optim:
  epoch: 3
  warmup_steps: 100
  lr: 0.0002
  lr_min: 0.00002
  beta1: 0.9
  beta2: 0.98
  max_norm: 1.0
  summary_interval: 100

s2u:
  dense_model_name: "hubert-base-ls960"
  quantizer_model_name: "kmeans"
  vocab_size: 100

  tokenizer_path: "models/speechlm/hubert/tokenizer.json"

  num_workers: 16